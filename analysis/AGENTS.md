# AGENTS.md

This document defines how the **Agent** should process conversation logs (JSONL format) generated by Claude Code or similar code-oriented LLMs.
The Agent must produce multiple human-readable reports **plus a `.json` file containing all raw quantitative data.**

- **Document all steps needed to repeat this analysis on a new conversation.**
- **Ask the user if anything is unclear or if there are important decisions that need to be made.**

---

# **1. Input Format**

- The Agent receives a **parent directory** `<parent_dir>`. Each **subdirectory inside `<parent_dir>` represents one independent conversation**.
- The Agent must process each conversation subdirectory **independently and in full**, producing a complete set of outputs *inside that subdirectory*.
- The Agent must **ignore any files outside of `<parent_dir>`**.

---

## **1.1 Conversation Directory Structure**

Example:

```
<parent_dir>/
    sub1/
	convo.jsonl
    sub2/
	convo.jsonl
    sub3/
        convo1.jsonl
	convo2.jsonl
```

Rules:

* Every first-level subdirectory is a **separate conversation**.
* All `.jsonl` files in that subdirectory belong to that conversation.
* JSONL files from different conversations must **never** be mixed.
* Output files must be written **inside the corresponding subdirectory**.

---

## **1.2 JSONL Record Format**

Within each conversation subdirectory, the Agent will receive one or more `.jsonl` files, where each record may include:

* `role`: "user", "assistant", or "tool"
* `content`: text or structured metadata
* tool call blocks (name, arguments, outputs)
* token usage (if present)
* timestamp (if present)

Missing fields must be handled gracefully and noted in the output.

---

# **2. Code**

- Any python code written should take as input the subdirectory.
- It should be general-purpose and applicable to any subdirectory.
- It should be stored in `<parent_dir>`.
- You may use the conda environment decoder-data-format to run any python code. Do not install anything without discussing with the user. 

# **3. Analysis**

Each subdirectory should be analyzed **identically** and **independently**. 

# **4. Output Files**

All output files should be placed in the input directory <dir>. The Agent must produce **these files**:

### **Markdown Reports (Human-Readable)**

| File                           | Description                                              |
| ------------------------------ | -------------------------------------------------------- |
| **01_intervention_summary.md** | Categorized list of all user interventions               |
| **02_work_summary.md**         | Step-by-step summary of assistant actions and tool calls |
| **03_token_usage.md**          | Token distribution and statistics                        |
| **04_timing.md**               | Timing distribution and statistics                       |

### **Single Consolidated JSON File (Raw Data)**

| File              | Description                                                                 |
| ----------------- | --------------------------------------------------------------------------- |
| **all_data.json** | A single JSON containing *all raw extracted and computed quantitative data* |

### **File Index**

| File                   | Description                                   |
| ---------------------- | --------------------------------------------- |
| **05_file_index.json** | Machine-readable manifest of all output files |

---

# **5. Consolidated data JSON file (`all_data.json`)**

The JSON file must contain a standard JSON object with the following top-level keys:

```
{
    "steps": [...],                  "# raw step-level entries"
    "interventions": [...],          # raw user intervention entries
    "tokens_per_step": {...},        # numeric token counts
    "tokens_per_tool": {...},        "# grouped token counts"
    "timing_per_step": {...},        # raw timing data
    "timing_per_tool": {...},        # grouped timing statistics
    "meta": {...}                    # versioning, parsing info, warnings
}
```

### **Detailed Requirements**

#### **steps**

Each entry must include the following fields, including the additional **tool type** key:

A list of dictionaries. Each entry contains:

* step number
* role
* content (raw)
* summarized content (if long)
* tool call metadata
* tool type (if applicable)
* token count
* timestamps (if present)

#### **interventions**

A list of dictionaries. Each entry contains:

* raw user message (quoted)
* categories (list)
* step number that preceded it
* short explanation of what triggered it
* whether it caused a correction or branch

#### **tokens_per_step**

A dict keyed by step number → numeric token count.

#### **tokens_per_tool**

A dict keyed by tool name → list of token counts.

#### **timing_per_step**

A dict keyed by step number →

* start timestamp
* end timestamp
* duration (seconds)

#### **timing_per_tool**

A dict keyed by tool name →

* list of durations
* aggregated stats (mean, median, std, min, max)

#### **meta**

Optional metadata:

* data_version
* agent_version
* warnings (missing timestamps, missing token data)
* input file names

---

# **6. Intervention Summary (01_intervention_summary.md)**

Use `all_data.json` to create a human-readable intervention summary. For each user message:

### Required fields

* Direct quote of message
* Categories (multi-label allowed)
* Step that came before it
* Triggering cause (one-sentence summary)
* Notes on corrections or redirections

### Official Categories

1. **Validation Flag / Correction Requested**
2. **Instruction Non-Compliance**
3. **User Question**
4. **Workflow Progression / Next Step**
5. **Spec Refinement**
6. **Result Review / Approval or Rejection**
7. **Meta Request**
8. **Out-of-Band Content**

The Agent may add more if justified and will list them.

---

# **7. Work Summary (02_work_summary.md)**

Use `all_data.json` to create a human-readable work summary. Include **every assistant step** in order.

### Required fields

* Step number
* Short title
* Quoted content (if short) or summary (if long)
* Tool information (tool name, arguments, purpose)
* Token count
* Timestamps
* Complexity tags (optional)

This file is human-readable — full raw data goes into `all_data.json`.

---

# **8. Token Usage Analysis (03_token_usage.md)**

Use `all_data.json` to compute and present:

### Per-Step Tokens

Markdown table:

* step number
* token count
* cumulative sum

### Tool-Level Statistics

Use `all_data.json` to compute and present, for each tool:

* mean
* median
* standard deviation
* min
* max
* number of calls

All underlying arrays must be saved in `all_data.json`.

---

# **9. Timing Analysis (04_timing.md)**

If timestamps exist, use `all_data.json` to compute and present:

### Per-Step Timing

* start time
* end time
* duration

### Per-Tool Timing

* mean
* median
* std
* min
* max
* total duration

### Overall

* total session runtime
* assistant active vs idle time

If timestamps **do not exist**, the file must contain:

> **Timing data unavailable: no timestamp fields found.**

Raw timing arrays and stats must be saved inside `all_data.json`.

---

# **8. File Index (05_file_index.json)**

A manifest of all output files.

Example:

```json
{
  "markdown_outputs": [
    "01_intervention_summary.md",
    "02_work_summary.md",
    "03_token_usage.md",
    "04_timing.md"
  ],
  "JSON": "all_data.json",
  "generated_at": "2025-01-01T12:00:00Z"
}
```

---

# **10. Additional Rules**

* All markdown files must be human-readable and well structured.
* Save any python code produced.
* Quotes of user messages should preserve formatting exactly.
* The JSON must include **everything quantifiable** — nothing should be lost.
* If multiple JSONL files are given, data must be merged chronologically.
* The agent should note any anomalies (missing fields, disordered timestamps, etc.) in the “meta” section of the JSON.
